
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import shutil
import json
import argparse
import logging
import hashlib
import asyncio
import aiofiles
from typing import AsyncGenerator, Tuple, Dict, Any, List
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm.asyncio import tqdm
from tqdm import tqdm as sync_tqdm
from colorama import init, Fore, Style

# å¯¼å…¥æ‰¹å¤„ç†ä»»åŠ¡æ± 
try:
    from batch_task_pool import BatchTaskPool
except ImportError:
    # å¦‚æœæ‰¾ä¸åˆ°BatchTaskPoolï¼Œåˆ›å»ºä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬
    class BatchTaskPool:
        """ç®€åŒ–ç‰ˆä»»åŠ¡æ± ï¼Œç”¨äºå¹¶å‘å¤„ç†"""
        def __init__(self, max_concurrent=8192):
            self.semaphore = asyncio.Semaphore(max_concurrent)
            self.active_tasks = {}
            self.task_counter = 0
            self.completed_count = 0
            self.failed_count = 0

        async def submit_task(self, coro, task_data):
            await self.semaphore.acquire()
            task_id = f"task_{self.task_counter}"
            self.task_counter += 1

            task = asyncio.create_task(self._execute_task(coro, task_id))
            self.active_tasks[task_id] = task
            return task_id, task

        async def _execute_task(self, coro, task_id):
            try:
                result = await coro
                self.completed_count += 1
                return result
            except Exception as e:
                self.failed_count += 1
                return {"status": "error", "error": str(e)}
            finally:
                self.semaphore.release()
                if task_id in self.active_tasks:
                    del self.active_tasks[task_id]

        def get_stats(self):
            total = self.completed_count + self.failed_count
            success_rate = (self.completed_count / total * 100) if total > 0 else 0
            return {
                "total_submitted": self.task_counter,
                "completed": self.completed_count,
                "failed": self.failed_count,
                "success_rate": success_rate
            }

# åˆå§‹åŒ–colorama
init(autoreset=True)

# æ”¯æŒçš„å›¾ç‰‡æ–‡ä»¶æ‰©å±•å
IMAGE_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.bmp', '.webp', '.gif', '.tiff', '.tif')

def find_image_json_pairs(source_dir):
    """
    é€’å½’æ‰«ææºç›®å½•ï¼ŒæŸ¥æ‰¾å›¾ç‰‡æ–‡ä»¶åŠå…¶å¯¹åº”çš„JSONæ–‡ä»¶ã€‚
    
    Args:
        source_dir (str): è¦æ‰«æçš„æºç›®å½•ã€‚
        
    Returns:
        list: åŒ…å«(å›¾ç‰‡è·¯å¾„, JSONè·¯å¾„)å…ƒç»„çš„åˆ—è¡¨ã€‚
    """
    pairs = []
    print(f"{Fore.BLUE}ğŸ” æ­£åœ¨æ‰«ææºç›®å½•: {source_dir}...{Style.RESET_ALL}")
    
    all_files = []
    for root, _, files in os.walk(source_dir):
        for file in files:
            all_files.append(os.path.join(root, file))

    image_files = [f for f in all_files if f.lower().endswith(IMAGE_EXTENSIONS)]
    
    print(f"{Fore.GREEN}ğŸ–¼ï¸  å‘ç° {len(image_files)} å¼ å›¾ç‰‡ã€‚æ­£åœ¨åŒ¹é…JSONæ–‡ä»¶...{Style.RESET_ALL}")

    for img_path in image_files:
        json_path = os.path.splitext(img_path)[0] + '.json'
        if os.path.exists(json_path):
            pairs.append((img_path, json_path))
        else:
            logging.warning(f"å›¾ç‰‡ {img_path} ç¼ºå°‘å¯¹åº”çš„JSONæ–‡ä»¶ï¼Œå·²è·³è¿‡ã€‚")
            
    print(f"{Fore.GREEN}âœ… æ‰¾åˆ° {len(pairs)} ä¸ªæœ‰æ•ˆçš„å›¾ç‰‡-JSONæ–‡ä»¶å¯¹ã€‚{Style.RESET_ALL}\n")
    return pairs

async def discover_image_json_pairs_streaming(source_dir: str) -> AsyncGenerator[Tuple[str, str], None]:
    """
    ä¼˜åŒ–çš„æµå¼å‘ç°å›¾ç‰‡-JSONæ–‡ä»¶å¯¹ï¼Œæ¶ˆé™¤åµŒå¥—å¾ªç¯

    ä¼˜åŒ–ç‰¹æ€§:
    - å•æ¬¡os.walkéå†ï¼Œä½¿ç”¨å­—å…¸æ˜ å°„é¿å…é‡å¤æœç´¢
    - é¢„æ„å»ºæ–‡ä»¶æ˜ å°„è¡¨ï¼Œæ¶ˆé™¤åµŒå¥—å¾ªç¯
    - å®æ—¶yieldæœ‰æ•ˆæ–‡ä»¶å¯¹ï¼Œæ”¯æŒæµå¼å¤„ç†
    - å¤§å¹…å‡å°‘å­—ç¬¦ä¸²æ“ä½œå’Œæ–‡ä»¶ç³»ç»Ÿè®¿é—®

    Args:
        source_dir: è¦æ‰«æçš„æºç›®å½•

    Yields:
        Tuple[str, str]: (å›¾ç‰‡è·¯å¾„, JSONè·¯å¾„)
    """
    # é¢„ç¼–è¯‘å›¾ç‰‡æ‰©å±•åé›†åˆï¼ˆåŒ…å«å¤§å°å†™å˜ä½“ï¼‰
    image_extensions = set()
    for ext in IMAGE_EXTENSIONS:
        image_extensions.add(ext.lower())
        image_extensions.add(ext.upper())

    discovered_count = 0

    try:
        # ä½¿ç”¨os.walkè¿›è¡Œé«˜æ•ˆé€’å½’éå†
        for root, dirs, files in os.walk(source_dir):
            # æ„å»ºæ–‡ä»¶æ˜ å°„è¡¨ï¼Œé¿å…é‡å¤çš„splitextæ“ä½œ
            file_map = {}  # base_name -> (full_path, extension)
            json_bases = set()  # JSONæ–‡ä»¶çš„åŸºç¡€åé›†åˆ
            
            # å•æ¬¡éå†æ„å»ºæ˜ å°„è¡¨
            for file in files:
                base_name, ext = os.path.splitext(file)
                full_path = os.path.join(root, file)
                
                if ext in image_extensions:
                    # å›¾ç‰‡æ–‡ä»¶ï¼šå­˜å‚¨åˆ°æ˜ å°„è¡¨
                    file_map[base_name] = (full_path, ext)
                elif ext.lower() == '.json':
                    # JSONæ–‡ä»¶ï¼šæ·»åŠ åˆ°é›†åˆå¹¶æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”å›¾ç‰‡
                    json_bases.add(base_name)
                    if base_name in file_map:
                        # ç«‹å³yieldæ‰¾åˆ°çš„é…å¯¹
                        img_path, _ = file_map[base_name]
                        json_path = full_path
                        discovered_count += 1
                        yield img_path, json_path
                        
                        # æ¯å‘ç°100ä¸ªæ–‡ä»¶å°±è®©å‡ºæ§åˆ¶æƒï¼Œä¿æŒå“åº”æ€§
                        if discovered_count % 100 == 0:
                            await asyncio.sleep(0)

            # å¤„ç†å…ˆé‡åˆ°å›¾ç‰‡åé‡åˆ°JSONçš„æƒ…å†µ
            for base_name, (img_path, _) in file_map.items():
                if base_name in json_bases:
                    continue  # å·²ç»å¤„ç†è¿‡
                # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„JSONæ–‡ä»¶
                json_path = os.path.join(root, base_name + '.json')
                if base_name + '.json' in [os.path.basename(f) for f in files if f.endswith('.json')]:
                    discovered_count += 1
                    yield img_path, json_path
                    
                    # æ¯å‘ç°100ä¸ªæ–‡ä»¶å°±è®©å‡ºæ§åˆ¶æƒï¼Œä¿æŒå“åº”æ€§
                    if discovered_count % 100 == 0:
                        await asyncio.sleep(0)

    except (PermissionError, OSError) as e:
        logging.warning(f"æ‰«æç›®å½•æ—¶é‡åˆ°é”™è¯¯: {e}")


def get_file_sha256(file_path):
    """è®¡ç®—æ–‡ä»¶çš„SHA256å“ˆå¸Œå€¼ï¼Œé€‚ç”¨äºå¤§æ–‡ä»¶ã€‚"""
    sha256 = hashlib.sha256()
    try:
        with open(file_path, "rb") as f:
            while True:
                # è¯»å–1MBçš„æ•°æ®å—
                data = f.read(1024 * 1024)
                if not data:
                    break
                sha256.update(data)
        return sha256.hexdigest()
    except IOError:
        logging.error(f"æ— æ³•è¯»å–æ–‡ä»¶è¿›è¡Œå“ˆå¸Œè®¡ç®—: {file_path}")
        return None

async def get_file_sha256_async(file_path: str) -> str:
    """å¼‚æ­¥è®¡ç®—æ–‡ä»¶çš„SHA256å“ˆå¸Œå€¼ï¼Œé€‚ç”¨äºå¤§æ–‡ä»¶ã€‚"""
    sha256 = hashlib.sha256()
    try:
        async with aiofiles.open(file_path, "rb") as f:
            while True:
                # è¯»å–1MBçš„æ•°æ®å—
                data = await f.read(1024 * 1024)
                if not data:
                    break
                sha256.update(data)
        return sha256.hexdigest()
    except IOError:
        logging.error(f"æ— æ³•è¯»å–æ–‡ä»¶è¿›è¡Œå“ˆå¸Œè®¡ç®—: {file_path}")
        return None

def evaluate_conditions(data, args):
    """
    æ ¹æ®ä¼ å…¥çš„å‚æ•°è¯„ä¼°å•ä¸ªæ•°æ®å¯¹è±¡æ˜¯å¦æ»¡è¶³æ‰€æœ‰ç­›é€‰æ¡ä»¶ã€‚
    
    Args:
        data (dict): ä»JSONæ–‡ä»¶è¯»å–çš„æ•°æ®ã€‚
        args (argparse.Namespace): è§£æåçš„å‘½ä»¤è¡Œå‚æ•°ã€‚
        
    Returns:
        bool: å¦‚æœæ»¡è¶³æ¡ä»¶åˆ™è¿”å›Trueï¼Œå¦åˆ™è¿”å›Falseã€‚
    """
    conditions = []
    
    # 1. åˆ†æ•°æ¡ä»¶
    if args.score:
        try:
            score_val = float(data.get('score', -1))
            if score_val == -1:
                raise KeyError
                
            parts = args.score.replace(' ', '').split(':')
            op = parts[0]
            
            if op == 'between':
                min_val, max_val = float(parts[1]), float(parts[2])
                conditions.append(min_val <= score_val <= max_val)
            else:
                val = float(parts[1])
                if op == '>': conditions.append(score_val > val)
                elif op == '<': conditions.append(score_val < val)
                elif op == '==': conditions.append(score_val == val)
                elif op == '>=': conditions.append(score_val >= val)
                elif op == '<=': conditions.append(score_val <= val)
        except (ValueError, IndexError):
            logging.error(f"æ— æ•ˆçš„åˆ†æ•°å‚æ•°æ ¼å¼: {args.score}ã€‚å·²è·³è¿‡æ­¤æ¡ä»¶ã€‚")
            conditions.append(False)
        except KeyError:
            logging.warning(f"JSONæ•°æ®ä¸­ç¼ºå°‘ 'score' å­—æ®µã€‚å·²è·³è¿‡æ­¤æ–‡ä»¶ã€‚")
            return False # ç›´æ¥åˆ¤å®šå¤±è´¥

    # 2. AIç”Ÿæˆæ¡ä»¶
    if args.is_ai is not None:
        try:
            ai_val = bool(data['is_ai_generated'])
            expected_val = args.is_ai == 'true'
            conditions.append(ai_val == expected_val)
        except KeyError:
            logging.warning(f"JSONæ•°æ®ä¸­ç¼ºå°‘ 'is_ai_generated' å­—æ®µã€‚å·²è·³è¿‡æ­¤æ–‡ä»¶ã€‚")
            return False

    # 3. æ°´å°æ¡ä»¶
    if args.has_watermark is not None:
        try:
            watermark_val = bool(data['watermark_present'])
            expected_val = args.has_watermark == 'true'
            conditions.append(watermark_val == expected_val)
        except KeyError:
            logging.warning(f"JSONæ•°æ®ä¸­ç¼ºå°‘ 'watermark_present' å­—æ®µã€‚å·²è·³è¿‡æ­¤æ–‡ä»¶ã€‚")
            return False

    if not conditions:
        return False # å¦‚æœæ²¡æœ‰ä»»ä½•ç­›é€‰æ¡ä»¶ï¼Œåˆ™é»˜è®¤ä¸åŒ¹é…

    if args.logic == 'AND':
        return all(conditions)
    else: # OR
        return any(conditions)

def process_image_sync(img_path: str, json_path: str, args) -> Dict[str, Any]:
    """
    åŒæ­¥å¤„ç†å•ä¸ªå›¾ç‰‡-JSONæ–‡ä»¶å¯¹ï¼Œä¼˜åŒ–ç”¨äºå¤šçº¿ç¨‹ç¯å¢ƒ
    
    Args:
        img_path: å›¾ç‰‡æ–‡ä»¶è·¯å¾„
        json_path: JSONæ–‡ä»¶è·¯å¾„
        args: å‘½ä»¤è¡Œå‚æ•°
        
    Returns:
        Dict: å¤„ç†ç»“æœ {"status": str, "path": str, "details": str}
    """
    try:
        # åŒæ­¥è¯»å–JSONæ–‡ä»¶
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        if evaluate_conditions(data, args):
            if not args.dry_run:
                if args.flat_output:
                    # å¹³é“ºè¾“å‡ºæ¨¡å¼ï¼šä½¿ç”¨SHA256é‡å‘½åå¹¶å¤åˆ¶åˆ°æ ¹ç›®å½•
                    img_hash = get_file_sha256(img_path)
                    if not img_hash:
                        return {"status": "error", "path": img_path, "details": "å“ˆå¸Œè®¡ç®—å¤±è´¥"}

                    _, img_ext = os.path.splitext(img_path)
                    
                    dest_img_path = os.path.join(args.dest, f"{img_hash}{img_ext}")
                    dest_json_path = os.path.join(args.dest, f"{img_hash}.json")
                    
                    # ä»…åˆ›å»ºç›®æ ‡æ ¹ç›®å½•ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
                    os.makedirs(args.dest, exist_ok=True)
                    
                    # åŒæ­¥å¤åˆ¶æ–‡ä»¶
                    shutil.copy2(img_path, dest_img_path)
                    shutil.copy2(json_path, dest_json_path)

                else:
                    # é»˜è®¤æ¨¡å¼ï¼šä¿æŒç›®å½•ç»“æ„
                    relative_path = os.path.relpath(img_path, args.source)
                    dest_img_path = os.path.join(args.dest, relative_path)
                    dest_json_path = os.path.splitext(dest_img_path)[0] + '.json'
                    
                    # åˆ›å»ºç›®æ ‡ç›®å½•ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
                    dest_dir = os.path.dirname(dest_img_path)
                    if dest_dir:
                        os.makedirs(dest_dir, exist_ok=True)
                    
                    # åŒæ­¥å¤åˆ¶æ–‡ä»¶
                    shutil.copy2(img_path, dest_img_path)
                    shutil.copy2(json_path, dest_json_path)

            return {"status": "copied", "path": img_path, "details": "æˆåŠŸå¤åˆ¶"}
        else:
            return {"status": "skipped", "path": img_path, "details": "ä¸æ»¡è¶³ç­›é€‰æ¡ä»¶"}
            
    except json.JSONDecodeError:
        return {"status": "error", "path": json_path, "details": "JSONæ ¼å¼é”™è¯¯"}
    except Exception as e:
        logging.error(f"å¤„ç† {img_path} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        return {"status": "error", "path": img_path, "details": str(e)}

def process_image(img_path, json_path, args):
    """
    å¤„ç†å•ä¸ªå›¾ç‰‡-JSONæ–‡ä»¶å¯¹ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
    
    Args:
        img_path (str): å›¾ç‰‡æ–‡ä»¶è·¯å¾„ã€‚
        json_path (str): JSONæ–‡ä»¶è·¯å¾„ã€‚
        args (argparse.Namespace): å‘½ä»¤è¡Œå‚æ•°ã€‚
        
    Returns:
        tuple: (çŠ¶æ€, æ–‡ä»¶è·¯å¾„)
    """
    result = process_image_sync(img_path, json_path, args)
    return result["status"], result["path"]

async def process_image_async(img_path: str, json_path: str, args) -> Dict[str, Any]:
    """
    å¼‚æ­¥å¤„ç†å•ä¸ªå›¾ç‰‡-JSONæ–‡ä»¶å¯¹

    Args:
        img_path: å›¾ç‰‡æ–‡ä»¶è·¯å¾„
        json_path: JSONæ–‡ä»¶è·¯å¾„
        args: å‘½ä»¤è¡Œå‚æ•°

    Returns:
        Dict: å¤„ç†ç»“æœ {"status": str, "path": str, "details": str}
    """
    try:
        # ä½¿ç”¨aiofileså¼‚æ­¥è¯»å–JSONæ–‡ä»¶
        async with aiofiles.open(json_path, 'r', encoding='utf-8') as f:
            content = await f.read()
            data = json.loads(content)

        if evaluate_conditions(data, args):
            if not args.dry_run:
                if args.flat_output:
                    # å¹³é“ºè¾“å‡ºæ¨¡å¼ï¼šä½¿ç”¨SHA256é‡å‘½åå¹¶å¤åˆ¶åˆ°æ ¹ç›®å½•
                    img_hash = await get_file_sha256_async(img_path)
                    if not img_hash:
                        return {"status": "error", "path": img_path, "details": "å“ˆå¸Œè®¡ç®—å¤±è´¥"}

                    _, img_ext = os.path.splitext(img_path)

                    dest_img_path = os.path.join(args.dest, f"{img_hash}{img_ext}")
                    dest_json_path = os.path.join(args.dest, f"{img_hash}.json")

                    # ä»…åˆ›å»ºç›®æ ‡æ ¹ç›®å½•
                    os.makedirs(args.dest, exist_ok=True)

                    # å¼‚æ­¥å¤åˆ¶æ–‡ä»¶
                    await asyncio.gather(
                        copy_file_async(img_path, dest_img_path),
                        copy_file_async(json_path, dest_json_path)
                    )

                else:
                    # é»˜è®¤æ¨¡å¼ï¼šä¿æŒç›®å½•ç»“æ„
                    relative_path = os.path.relpath(img_path, args.source)
                    dest_img_path = os.path.join(args.dest, relative_path)
                    dest_json_path = os.path.splitext(dest_img_path)[0] + '.json'

                    # åˆ›å»ºç›®æ ‡ç›®å½•
                    os.makedirs(os.path.dirname(dest_img_path), exist_ok=True)

                    # å¼‚æ­¥å¤åˆ¶æ–‡ä»¶
                    await asyncio.gather(
                        copy_file_async(img_path, dest_img_path),
                        copy_file_async(json_path, dest_json_path)
                    )

            return {"status": "copied", "path": img_path, "details": "æˆåŠŸå¤åˆ¶"}
        else:
            return {"status": "skipped", "path": img_path, "details": "ä¸æ»¡è¶³ç­›é€‰æ¡ä»¶"}

    except json.JSONDecodeError:
        return {"status": "error", "path": json_path, "details": "JSONæ ¼å¼é”™è¯¯"}
    except Exception as e:
        logging.error(f"å¤„ç† {img_path} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        return {"status": "error", "path": img_path, "details": str(e)}

async def copy_file_async(src_path: str, dest_path: str):
    """å¼‚æ­¥å¤åˆ¶æ–‡ä»¶"""
    try:
        async with aiofiles.open(src_path, 'rb') as src:
            async with aiofiles.open(dest_path, 'wb') as dest:
                while True:
                    chunk = await src.read(1024 * 1024)  # 1MB chunks
                    if not chunk:
                        break
                    await dest.write(chunk)
    except Exception as e:
        logging.error(f"å¤åˆ¶æ–‡ä»¶å¤±è´¥ {src_path} -> {dest_path}: {e}")
        raise

def setup_logging(log_file):
    """é…ç½®æ—¥å¿—è®°å½•"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        filename=log_file,
        filemode='w'
    )
    # æ·»åŠ ä¸€ä¸ªæ§åˆ¶å°å¤„ç†å™¨ï¼Œç”¨äºæ˜¾ç¤ºè­¦å‘Šå’Œé”™è¯¯
    console = logging.StreamHandler()
    console.setLevel(logging.WARNING)
    formatter = logging.Formatter('%(levelname)s - %(message)s')
    console.setFormatter(formatter)
    logging.getLogger('').addHandler(console)

def setup_parser():
    """è®¾ç½®å’Œé…ç½®å‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(
        description='æ ¹æ®JSONåˆ†æç»“æœç­›é€‰å›¾ç‰‡å¹¶å¤åˆ¶æ–‡ä»¶ã€‚',
        formatter_class=argparse.RawTextHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  - ç­›é€‰åˆ†æ•°å¤§äº8.5åˆ†çš„éAIã€æ— æ°´å°å›¾ç‰‡:
    python %(prog)s --source ./images --dest ./high_quality --score '>:8.5' --is-ai false --has-watermark false

  - ç­›é€‰åˆ†æ•°åœ¨7åˆ°8ä¹‹é—´ï¼Œæˆ–è€…æ˜¯AIç”Ÿæˆçš„å›¾ç‰‡:
    python %(prog)s --source ./images --dest ./filtered --score 'between:7:8' --is-ai true --logic OR
    
  - æ¨¡æ‹Ÿè¿è¡Œï¼ŒæŸ¥çœ‹å°†è¦å¤åˆ¶çš„æ–‡ä»¶:
    python %(prog)s --source ./images --dest ./filtered --score '<:5' --dry-run
"""
    )
    
    # å¿…éœ€å‚æ•°
    parser.add_argument('--source', type=str, required=True, help='åŒ…å«å›¾ç‰‡å’ŒJSONæ–‡ä»¶çš„æºç›®å½•è·¯å¾„ã€‚')
    parser.add_argument('--dest', type=str, required=True, help='ç”¨äºå­˜æ”¾ç­›é€‰åæ–‡ä»¶çš„ç›®æ ‡ç›®å½•è·¯å¾„ã€‚')
    
    # å¯é€‰ç­›é€‰å‚æ•°
    parser.add_argument('--score', type=str, help="åˆ†æ•°ç­›é€‰æ¡ä»¶ã€‚æ ¼å¼: 'OP:VALUE' æˆ– 'between:MIN:MAX'ã€‚\næœ‰æ•ˆOP: '>', '<', '==', '>=', '<='ã€‚ç¤ºä¾‹: --score '>:8.5'")
    parser.add_argument('--is-ai', type=str, choices=['true', 'false'], help="AIç”ŸæˆçŠ¶æ€ç­›é€‰ã€‚'true' æˆ– 'false'ã€‚")
    parser.add_argument('--has-watermark', type=str, choices=['true', 'false'], help="æ°´å°çŠ¶æ€ç­›é€‰ã€‚'true' æˆ– 'false'ã€‚")
    
    # æ§åˆ¶å‚æ•°
    parser.add_argument('--logic', type=str, choices=['AND', 'OR'], default='AND', help="å¤šä¸ªç­›é€‰æ¡ä»¶ä¹‹é—´çš„é€»è¾‘å…³ç³» (é»˜è®¤: AND)ã€‚")
    parser.add_argument('--workers', type=int, default=os.cpu_count() * 4, help='å¹¶è¡Œå¤„ç†çš„çº¿ç¨‹æ•°é‡ (é»˜è®¤: CPUæ ¸å¿ƒæ•°*4ï¼Œæœ€å¤š16384ä¸ªçº¿ç¨‹)ã€‚')
    parser.add_argument('--dry-run', action='store_true', help='æ¨¡æ‹Ÿè¿è¡Œï¼Œåªæ‰“å°æ“ä½œä¿¡æ¯è€Œä¸å®é™…å¤åˆ¶æ–‡ä»¶ã€‚')
    parser.add_argument('--flat-output', action='store_true', help='å°†æ‰€æœ‰æ–‡ä»¶å¤åˆ¶åˆ°ç›®æ ‡ç›®å½•çš„æ ¹çº§åˆ«ï¼Œå¹¶ä»¥SHA256é‡å‘½åã€‚')
    parser.add_argument('--log-file', type=str, default='filter_log.txt', help='æŒ‡å®šæ—¥å¿—æ–‡ä»¶çš„è·¯å¾„ (é»˜è®¤: filter_log.txt)ã€‚')
    
    return parser

async def process_images_threaded(
    image_pairs: List[Tuple[str, str]],
    args,
    max_workers: int = None
) -> Dict[str, int]:
    """
    ä½¿ç”¨çº¿ç¨‹æ± å¤„ç†å›¾ç‰‡æ–‡ä»¶å¯¹ï¼Œä¼˜åŒ–I/Oå¯†é›†å‹æ“ä½œ

    Args:
        image_pairs: å›¾ç‰‡-JSONæ–‡ä»¶å¯¹åˆ—è¡¨
        args: å‘½ä»¤è¡Œå‚æ•°
        max_workers: æœ€å¤§çº¿ç¨‹æ•°ï¼Œé»˜è®¤ä¸ºCPUæ ¸å¿ƒæ•°*4

    Returns:
        Dict[str, int]: å¤„ç†ç»“æœç»Ÿè®¡
    """
    if not image_pairs:
        return {'copied': 0, 'skipped': 0, 'error': 0}

    # æ™ºèƒ½è®¾ç½®çº¿ç¨‹æ•°ï¼šI/Oå¯†é›†å‹æ“ä½œä½¿ç”¨æ›´å¤šçº¿ç¨‹
    if max_workers is None:
        max_workers = min(os.cpu_count() * 4, 16384)  # æœ€å¤š16384ä¸ªçº¿ç¨‹
    
    print(f"THREADS: {max_workers} workers")

    # åˆå§‹åŒ–è®¡æ•°å™¨
    success_count = 0
    skip_count = 0
    error_count = 0
    results = []

    # ä½¿ç”¨ThreadPoolExecutorå¤„ç†I/Oå¯†é›†å‹ä»»åŠ¡
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_pair = {
            executor.submit(process_image_sync, img_path, json_path, args): (img_path, json_path)
            for img_path, json_path in image_pairs
        }

        # ä½¿ç”¨tqdmæ˜¾ç¤ºå¤„ç†è¿›åº¦
        with sync_tqdm(total=len(image_pairs), desc="PROCESS", unit="pairs", ncols=80) as pbar:
            # ä½¿ç”¨as_completedè·å–å®Œæˆçš„ä»»åŠ¡ï¼Œé¿å…è½®è¯¢
            for future in as_completed(future_to_pair):
                try:
                    result = future.result()
                    results.append(result)

                    # æ›´æ–°è®¡æ•°å™¨
                    status = result.get("status", "unknown")
                    if status == "copied":
                        success_count += 1
                    elif status == "skipped":
                        skip_count += 1
                    else:
                        error_count += 1

                    # æ›´æ–°è¿›åº¦æ¡
                    pbar.update(1)
                    pbar.set_postfix_str(f"æˆåŠŸ={success_count}ï¼Œè·³è¿‡={skip_count}ï¼Œå¤±è´¥={error_count}")

                except Exception as e:
                    # å¤„ç†ä»»åŠ¡å¼‚å¸¸
                    img_path, json_path = future_to_pair[future]
                    error_result = {
                        "status": "error",
                        "path": img_path,
                        "details": f"çº¿ç¨‹æ‰§è¡Œé”™è¯¯: {str(e)}"
                    }
                    results.append(error_result)
                    error_count += 1
                    
                    pbar.update(1)
                    pbar.set_postfix_str(f"æˆåŠŸ={success_count}ï¼Œè·³è¿‡={skip_count}ï¼Œå¤±è´¥={error_count}")

    # ç»Ÿè®¡ç»“æœ
    result_counts = {'copied': 0, 'skipped': 0, 'error': 0}
    for result in results:
        status = result.get("status", "error")
        if status in result_counts:
            result_counts[status] += 1
        else:
            result_counts['error'] += 1

    # æ˜¾ç¤ºå¤„ç†ç»Ÿè®¡
    total_processed = len(results)
    success_rate = (success_count / total_processed * 100) if total_processed > 0 else 0
    print(f"STATS: {success_rate:.1f}% success ({success_count}/{total_processed})")

    return result_counts

async def process_images_concurrent(
    image_pairs: List[Tuple[str, str]],
    args,
    max_concurrent: int = 8192
) -> Dict[str, int]:
    """
    å¹¶å‘å¤„ç†å›¾ç‰‡æ–‡ä»¶å¯¹ï¼ˆä¿æŒå‘åå…¼å®¹ï¼Œä½†æ¨èä½¿ç”¨process_images_threadedï¼‰

    Args:
        image_pairs: å›¾ç‰‡-JSONæ–‡ä»¶å¯¹åˆ—è¡¨
        args: å‘½ä»¤è¡Œå‚æ•°
        max_concurrent: æœ€å¤§å¹¶å‘æ•°

    Returns:
        Dict[str, int]: å¤„ç†ç»“æœç»Ÿè®¡
    """
    # å¯¹äºå¤§é‡æ–‡ä»¶ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°çº¿ç¨‹æ± æ¨¡å¼
    if len(image_pairs) > 1000:
        print("æ£€æµ‹åˆ°å¤§é‡æ–‡ä»¶ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°ä¼˜åŒ–çš„çº¿ç¨‹æ± æ¨¡å¼...")
        return await process_images_threaded(image_pairs, args)
    
    # å°é‡æ–‡ä»¶ä¿æŒåŸæœ‰é€»è¾‘ï¼ˆä½†é™ä½å¹¶å‘æ•°ï¼‰
    max_concurrent = min(max_concurrent, 200)  # é™åˆ¶æœ€å¤§å¹¶å‘æ•°
    return await process_images_threaded(image_pairs, args, max_concurrent // 4)

async def main_async():
    """å¼‚æ­¥ä¸»å‡½æ•°ï¼Œå®ç°æµå¼è¿›åº¦æ¡å’Œå¹¶å‘å¤„ç†"""
    parser = setup_parser()
    args = parser.parse_args()
    
    # è·å–å®é™…çš„å¹¶å‘é…ç½® - æ”¹ä¸ºçº¿ç¨‹æ•°é…ç½®
    max_workers = int(os.getenv('IMAGE_FILTER_THREAD_WORKERS', str(os.cpu_count() * 4)))
    max_workers = min(max_workers, 16384)  # é™åˆ¶æœ€å¤§çº¿ç¨‹æ•°

    # DOSé£æ ¼é…ç½®æ˜¾ç¤º
    print("=" * 60)
    print("IMAGE FILTER v3.0 - THREADED EDITION")
    print("=" * 60)
    print(f"SOURCE: {args.source}")
    print(f"DEST  : {args.dest}")
    print(f"FILTER: {args.score or 'NONE'} | AI:{args.is_ai or 'ANY'} | WM:{args.has_watermark or 'ANY'}")
    print(f"THREADS: {max_workers} WORKERS")
    if args.dry_run:
        print("MODE  : DRY RUN (SIMULATION)")
    print("=" * 60)

    # é…ç½®æ—¥å¿—
    setup_logging(args.log_file)

    # æ£€æŸ¥ç­›é€‰æ¡ä»¶
    if not any([args.score, args.is_ai, args.has_watermark]):
        print("WARNING: No filter conditions specified!")
        return

    # æ–‡ä»¶å‘ç°é˜¶æ®µ
    print("\nSCANNING...")
    image_pairs = []
    discovered_count = 0

    # ä½¿ç”¨æµå¼å‘ç°å¹¶æ˜¾ç¤ºå®æ—¶è¿›åº¦
    with tqdm(desc="DISCOVER", unit="pairs", ncols=80) as discovery_pbar:
        async for img_path, json_path in discover_image_json_pairs_streaming(args.source):
            image_pairs.append((img_path, json_path))
            discovered_count += 1
            discovery_pbar.update(1)

            # æ¯å‘ç°1000ä¸ªæ–‡ä»¶å°±è®©å‡ºæ§åˆ¶æƒ
            if discovered_count % 1000 == 0:
                await asyncio.sleep(0)

    if not image_pairs:
        print("ERROR: No image-JSON pairs found!")
        return

    print(f"FOUND: {len(image_pairs)} pairs")

    # å¤„ç†é˜¶æ®µ
    print("PROCESSING...")
    results = await process_images_threaded(
        image_pairs,
        args,
        max_workers=max_workers
    )

    # ç»“æœç»Ÿè®¡
    print("\n" + "=" * 60)
    print("RESULTS:")
    print(f"COPIED : {results['copied']}")
    print(f"SKIPPED: {results['skipped']}")
    print(f"ERRORS : {results['error']}")
    print(f"LOG    : {args.log_file}")
    if args.dry_run:
        print("STATUS : SIMULATION COMPLETE")
    else:
        print("STATUS : OPERATION COMPLETE")
    print("=" * 60)

def main():
    """åŒæ­¥å…¥å£ç‚¹ï¼Œè°ƒç”¨å¼‚æ­¥ä¸»å‡½æ•°"""
    return asyncio.run(main_async())

if __name__ == "__main__":
    main()
