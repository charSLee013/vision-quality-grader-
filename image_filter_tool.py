#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import shutil
import json
import argparse
import logging
import hashlib
import asyncio
import aiofiles
from typing import AsyncGenerator, Tuple, Dict, Any, List
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm.asyncio import tqdm
from colorama import init, Fore, Style

# å¯¼å…¥æ‰¹å¤„ç†ä»»åŠ¡æ± 
try:
    from batch_task_pool import BatchTaskPool
except ImportError:
    # å¦‚æœæ‰¾ä¸åˆ°BatchTaskPoolï¼Œåˆ›å»ºä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬
    class BatchTaskPool:
        """ç®€åŒ–ç‰ˆä»»åŠ¡æ± ï¼Œç”¨äºå¹¶å‘å¤„ç†"""
        def __init__(self, max_concurrent=8192):
            self.semaphore = asyncio.Semaphore(max_concurrent)
            self.active_tasks = {}
            self.task_counter = 0
            self.completed_count = 0
            self.failed_count = 0

        async def submit_task(self, coro, task_data):
            await self.semaphore.acquire()
            task_id = f"task_{self.task_counter}"
            self.task_counter += 1

            task = asyncio.create_task(self._execute_task(coro, task_id))
            self.active_tasks[task_id] = task
            return task_id, task

        async def _execute_task(self, coro, task_id):
            try:
                result = await coro
                self.completed_count += 1
                return result
            except Exception as e:
                self.failed_count += 1
                return {"status": "error", "error": str(e)}
            finally:
                self.semaphore.release()
                if task_id in self.active_tasks:
                    del self.active_tasks[task_id]

        def get_stats(self):
            total = self.completed_count + self.failed_count
            success_rate = (self.completed_count / total * 100) if total > 0 else 0
            return {
                "total_submitted": self.task_counter,
                "completed": self.completed_count,
                "failed": self.failed_count,
                "success_rate": success_rate
            }

# åˆå§‹åŒ–colorama
init(autoreset=True)

# æ”¯æŒçš„å›¾ç‰‡æ–‡ä»¶æ‰©å±•å
IMAGE_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.bmp', '.webp', '.gif', '.tiff', '.tif')

def find_image_json_pairs(source_dir):
    """
    é€’å½’æ‰«ææºç›®å½•ï¼ŒæŸ¥æ‰¾å›¾ç‰‡æ–‡ä»¶åŠå…¶å¯¹åº”çš„JSONæ–‡ä»¶ã€‚
    
    Args:
        source_dir (str): è¦æ‰«æçš„æºç›®å½•ã€‚
        
    Returns:
        list: åŒ…å«(å›¾ç‰‡è·¯å¾„, JSONè·¯å¾„)å…ƒç»„çš„åˆ—è¡¨ã€‚
    """
    pairs = []
    print(f"{Fore.BLUE}ğŸ” æ­£åœ¨æ‰«ææºç›®å½•: {source_dir}...{Style.RESET_ALL}")
    
    all_files = []
    for root, _, files in os.walk(source_dir):
        for file in files:
            all_files.append(os.path.join(root, file))

    image_files = [f for f in all_files if f.lower().endswith(IMAGE_EXTENSIONS)]
    
    print(f"{Fore.GREEN}ğŸ–¼ï¸  å‘ç° {len(image_files)} å¼ å›¾ç‰‡ã€‚æ­£åœ¨åŒ¹é…JSONæ–‡ä»¶...{Style.RESET_ALL}")

    for img_path in image_files:
        json_path = os.path.splitext(img_path)[0] + '.json'
        if os.path.exists(json_path):
            pairs.append((img_path, json_path))
        else:
            logging.warning(f"å›¾ç‰‡ {img_path} ç¼ºå°‘å¯¹åº”çš„JSONæ–‡ä»¶ï¼Œå·²è·³è¿‡ã€‚")
            
    print(f"{Fore.GREEN}âœ… æ‰¾åˆ° {len(pairs)} ä¸ªæœ‰æ•ˆçš„å›¾ç‰‡-JSONæ–‡ä»¶å¯¹ã€‚{Style.RESET_ALL}\n")
    return pairs

async def discover_image_json_pairs_streaming(source_dir: str) -> AsyncGenerator[Tuple[str, str], None]:
    """
    æµå¼å‘ç°å›¾ç‰‡-JSONæ–‡ä»¶å¯¹ï¼Œå•æ¬¡æ–‡ä»¶ç³»ç»Ÿéå†

    ä¼˜åŒ–ç‰¹æ€§:
    - å•æ¬¡os.walkéå†åŒæ—¶è¯†åˆ«å›¾ç‰‡å’ŒJSONæ–‡ä»¶
    - å®æ—¶yieldæœ‰æ•ˆæ–‡ä»¶å¯¹ï¼Œæ”¯æŒæµå¼å¤„ç†
    - é¿å…æ„å»ºå¤§å‹æ–‡ä»¶åˆ—è¡¨ï¼Œå‡å°‘å†…å­˜ä½¿ç”¨

    Args:
        source_dir: è¦æ‰«æçš„æºç›®å½•

    Yields:
        Tuple[str, str]: (å›¾ç‰‡è·¯å¾„, JSONè·¯å¾„)
    """
    # å›¾ç‰‡æ‰©å±•åé›†åˆï¼ˆåŒ…å«å¤§å°å†™å˜ä½“ï¼‰
    image_extensions = set()
    for ext in IMAGE_EXTENSIONS:
        image_extensions.add(ext.lower())
        image_extensions.add(ext.upper())

    discovered_count = 0

    try:
        # ä½¿ç”¨os.walkè¿›è¡Œé«˜æ•ˆé€’å½’éå†
        for root, dirs, files in os.walk(source_dir):
            # åœ¨å½“å‰ç›®å½•ä¸­æŸ¥æ‰¾å›¾ç‰‡-JSONæ–‡ä»¶å¯¹
            image_files = set()
            json_files = set()

            # åˆ†ç±»æ–‡ä»¶
            for file in files:
                file_path = os.path.join(root, file)
                _, ext = os.path.splitext(file)

                if ext in image_extensions:
                    image_files.add(os.path.splitext(file)[0])  # ä¸å¸¦æ‰©å±•åçš„åŸºç¡€å
                elif ext.lower() == '.json':
                    json_files.add(os.path.splitext(file)[0])   # ä¸å¸¦æ‰©å±•åçš„åŸºç¡€å

            # æ‰¾åˆ°åŒ¹é…çš„å›¾ç‰‡-JSONå¯¹
            matching_pairs = image_files.intersection(json_files)

            for base_name in matching_pairs:
                img_path = None
                json_path = os.path.join(root, base_name + '.json')

                # æ‰¾åˆ°å¯¹åº”çš„å›¾ç‰‡æ–‡ä»¶ï¼ˆå¯èƒ½æœ‰ä¸åŒæ‰©å±•åï¼‰
                for file in files:
                    if os.path.splitext(file)[0] == base_name and os.path.splitext(file)[1] in image_extensions:
                        img_path = os.path.join(root, file)
                        break

                if img_path:
                    discovered_count += 1
                    yield img_path, json_path

                    # æ¯å‘ç°100ä¸ªæ–‡ä»¶å°±è®©å‡ºæ§åˆ¶æƒï¼Œä¿æŒå“åº”æ€§
                    if discovered_count % 100 == 0:
                        await asyncio.sleep(0)

    except (PermissionError, OSError) as e:
        logging.warning(f"æ‰«æç›®å½•æ—¶é‡åˆ°é”™è¯¯: {e}")

def get_file_sha256(file_path):
    """è®¡ç®—æ–‡ä»¶çš„SHA256å“ˆå¸Œå€¼ï¼Œé€‚ç”¨äºå¤§æ–‡ä»¶ã€‚"""
    sha256 = hashlib.sha256()
    try:
        with open(file_path, "rb") as f:
            while True:
                # è¯»å–1MBçš„æ•°æ®å—
                data = f.read(1024 * 1024)
                if not data:
                    break
                sha256.update(data)
        return sha256.hexdigest()
    except IOError:
        logging.error(f"æ— æ³•è¯»å–æ–‡ä»¶è¿›è¡Œå“ˆå¸Œè®¡ç®—: {file_path}")
        return None

async def get_file_sha256_async(file_path: str) -> str:
    """å¼‚æ­¥è®¡ç®—æ–‡ä»¶çš„SHA256å“ˆå¸Œå€¼ï¼Œé€‚ç”¨äºå¤§æ–‡ä»¶ã€‚"""
    sha256 = hashlib.sha256()
    try:
        async with aiofiles.open(file_path, "rb") as f:
            while True:
                # è¯»å–1MBçš„æ•°æ®å—
                data = await f.read(1024 * 1024)
                if not data:
                    break
                sha256.update(data)
        return sha256.hexdigest()
    except IOError:
        logging.error(f"æ— æ³•è¯»å–æ–‡ä»¶è¿›è¡Œå“ˆå¸Œè®¡ç®—: {file_path}")
        return None

def evaluate_conditions(data, args):
    """
    æ ¹æ®ä¼ å…¥çš„å‚æ•°è¯„ä¼°å•ä¸ªæ•°æ®å¯¹è±¡æ˜¯å¦æ»¡è¶³æ‰€æœ‰ç­›é€‰æ¡ä»¶ã€‚
    
    Args:
        data (dict): ä»JSONæ–‡ä»¶è¯»å–çš„æ•°æ®ã€‚
        args (argparse.Namespace): è§£æåçš„å‘½ä»¤è¡Œå‚æ•°ã€‚
        
    Returns:
        bool: å¦‚æœæ»¡è¶³æ¡ä»¶åˆ™è¿”å›Trueï¼Œå¦åˆ™è¿”å›Falseã€‚
    """
    conditions = []
    
    # 1. åˆ†æ•°æ¡ä»¶
    if args.score:
        try:
            score_val = float(data.get('score', -1))
            if score_val == -1:
                raise KeyError
                
            parts = args.score.replace(' ', '').split(':')
            op = parts[0]
            
            if op == 'between':
                min_val, max_val = float(parts[1]), float(parts[2])
                conditions.append(min_val <= score_val <= max_val)
            else:
                val = float(parts[1])
                if op == '>': conditions.append(score_val > val)
                elif op == '<': conditions.append(score_val < val)
                elif op == '==': conditions.append(score_val == val)
                elif op == '>=': conditions.append(score_val >= val)
                elif op == '<=': conditions.append(score_val <= val)
        except (ValueError, IndexError):
            logging.error(f"æ— æ•ˆçš„åˆ†æ•°å‚æ•°æ ¼å¼: {args.score}ã€‚å·²è·³è¿‡æ­¤æ¡ä»¶ã€‚")
            conditions.append(False)
        except KeyError:
            logging.warning(f"JSONæ•°æ®ä¸­ç¼ºå°‘ 'score' å­—æ®µã€‚å·²è·³è¿‡æ­¤æ–‡ä»¶ã€‚")
            return False # ç›´æ¥åˆ¤å®šå¤±è´¥

    # 2. AIç”Ÿæˆæ¡ä»¶
    if args.is_ai is not None:
        try:
            ai_val = bool(data['is_ai_generated'])
            expected_val = args.is_ai == 'true'
            conditions.append(ai_val == expected_val)
        except KeyError:
            logging.warning(f"JSONæ•°æ®ä¸­ç¼ºå°‘ 'is_ai_generated' å­—æ®µã€‚å·²è·³è¿‡æ­¤æ–‡ä»¶ã€‚")
            return False

    # 3. æ°´å°æ¡ä»¶
    if args.has_watermark is not None:
        try:
            watermark_val = bool(data['watermark_present'])
            expected_val = args.has_watermark == 'true'
            conditions.append(watermark_val == expected_val)
        except KeyError:
            logging.warning(f"JSONæ•°æ®ä¸­ç¼ºå°‘ 'watermark_present' å­—æ®µã€‚å·²è·³è¿‡æ­¤æ–‡ä»¶ã€‚")
            return False

    if not conditions:
        return False # å¦‚æœæ²¡æœ‰ä»»ä½•ç­›é€‰æ¡ä»¶ï¼Œåˆ™é»˜è®¤ä¸åŒ¹é…

    if args.logic == 'AND':
        return all(conditions)
    else: # OR
        return any(conditions)

def process_image(img_path, json_path, args):
    """
    å¤„ç†å•ä¸ªå›¾ç‰‡-JSONæ–‡ä»¶å¯¹ã€‚
    
    Args:
        img_path (str): å›¾ç‰‡æ–‡ä»¶è·¯å¾„ã€‚
        json_path (str): JSONæ–‡ä»¶è·¯å¾„ã€‚
        args (argparse.Namespace): å‘½ä»¤è¡Œå‚æ•°ã€‚
        
    Returns:
        tuple: (çŠ¶æ€, æ–‡ä»¶è·¯å¾„)
    """
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        if evaluate_conditions(data, args):
            if not args.dry_run:
                if args.flat_output:
                    # å¹³é“ºè¾“å‡ºæ¨¡å¼ï¼šä½¿ç”¨SHA256é‡å‘½åå¹¶å¤åˆ¶åˆ°æ ¹ç›®å½•
                    img_hash = get_file_sha256(img_path)
                    if not img_hash:
                        return 'error', img_path # å“ˆå¸Œè®¡ç®—å¤±è´¥

                    _, img_ext = os.path.splitext(img_path)
                    
                    dest_img_path = os.path.join(args.dest, f"{img_hash}{img_ext}")
                    dest_json_path = os.path.join(args.dest, f"{img_hash}.json")
                    
                    # ä»…åˆ›å»ºç›®æ ‡æ ¹ç›®å½•
                    os.makedirs(args.dest, exist_ok=True)
                    
                    # å¤åˆ¶æ–‡ä»¶
                    shutil.copy2(img_path, dest_img_path)
                    shutil.copy2(json_path, dest_json_path)

                else:
                    # é»˜è®¤æ¨¡å¼ï¼šä¿æŒç›®å½•ç»“æ„
                    relative_path = os.path.relpath(img_path, args.source)
                    dest_img_path = os.path.join(args.dest, relative_path)
                    dest_json_path = os.path.splitext(dest_img_path)[0] + '.json'
                    
                    # åˆ›å»ºç›®æ ‡ç›®å½•
                    os.makedirs(os.path.dirname(dest_img_path), exist_ok=True)
                    
                    # å¤åˆ¶æ–‡ä»¶
                    shutil.copy2(img_path, dest_img_path)
                    shutil.copy2(json_path, dest_json_path)

            return 'copied', img_path
        else:
            return 'skipped', img_path
            
    except json.JSONDecodeError:
        return 'error', json_path
    except Exception as e:
        logging.error(f"å¤„ç† {img_path} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        return 'error', img_path

async def process_image_async(img_path: str, json_path: str, args) -> Dict[str, Any]:
    """
    å¼‚æ­¥å¤„ç†å•ä¸ªå›¾ç‰‡-JSONæ–‡ä»¶å¯¹

    Args:
        img_path: å›¾ç‰‡æ–‡ä»¶è·¯å¾„
        json_path: JSONæ–‡ä»¶è·¯å¾„
        args: å‘½ä»¤è¡Œå‚æ•°

    Returns:
        Dict: å¤„ç†ç»“æœ {"status": str, "path": str, "details": str}
    """
    try:
        # ä½¿ç”¨aiofileså¼‚æ­¥è¯»å–JSONæ–‡ä»¶
        async with aiofiles.open(json_path, 'r', encoding='utf-8') as f:
            content = await f.read()
            data = json.loads(content)

        if evaluate_conditions(data, args):
            if not args.dry_run:
                if args.flat_output:
                    # å¹³é“ºè¾“å‡ºæ¨¡å¼ï¼šä½¿ç”¨SHA256é‡å‘½åå¹¶å¤åˆ¶åˆ°æ ¹ç›®å½•
                    img_hash = await get_file_sha256_async(img_path)
                    if not img_hash:
                        return {"status": "error", "path": img_path, "details": "å“ˆå¸Œè®¡ç®—å¤±è´¥"}

                    _, img_ext = os.path.splitext(img_path)

                    dest_img_path = os.path.join(args.dest, f"{img_hash}{img_ext}")
                    dest_json_path = os.path.join(args.dest, f"{img_hash}.json")

                    # ä»…åˆ›å»ºç›®æ ‡æ ¹ç›®å½•
                    os.makedirs(args.dest, exist_ok=True)

                    # å¼‚æ­¥å¤åˆ¶æ–‡ä»¶
                    await asyncio.gather(
                        copy_file_async(img_path, dest_img_path),
                        copy_file_async(json_path, dest_json_path)
                    )

                else:
                    # é»˜è®¤æ¨¡å¼ï¼šä¿æŒç›®å½•ç»“æ„
                    relative_path = os.path.relpath(img_path, args.source)
                    dest_img_path = os.path.join(args.dest, relative_path)
                    dest_json_path = os.path.splitext(dest_img_path)[0] + '.json'

                    # åˆ›å»ºç›®æ ‡ç›®å½•
                    os.makedirs(os.path.dirname(dest_img_path), exist_ok=True)

                    # å¼‚æ­¥å¤åˆ¶æ–‡ä»¶
                    await asyncio.gather(
                        copy_file_async(img_path, dest_img_path),
                        copy_file_async(json_path, dest_json_path)
                    )

            return {"status": "copied", "path": img_path, "details": "æˆåŠŸå¤åˆ¶"}
        else:
            return {"status": "skipped", "path": img_path, "details": "ä¸æ»¡è¶³ç­›é€‰æ¡ä»¶"}

    except json.JSONDecodeError:
        return {"status": "error", "path": json_path, "details": "JSONæ ¼å¼é”™è¯¯"}
    except Exception as e:
        logging.error(f"å¤„ç† {img_path} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        return {"status": "error", "path": img_path, "details": str(e)}

async def copy_file_async(src_path: str, dest_path: str):
    """å¼‚æ­¥å¤åˆ¶æ–‡ä»¶"""
    try:
        async with aiofiles.open(src_path, 'rb') as src:
            async with aiofiles.open(dest_path, 'wb') as dest:
                while True:
                    chunk = await src.read(1024 * 1024)  # 1MB chunks
                    if not chunk:
                        break
                    await dest.write(chunk)
    except Exception as e:
        logging.error(f"å¤åˆ¶æ–‡ä»¶å¤±è´¥ {src_path} -> {dest_path}: {e}")
        raise

def setup_logging(log_file):
    """é…ç½®æ—¥å¿—è®°å½•"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        filename=log_file,
        filemode='w'
    )
    # æ·»åŠ ä¸€ä¸ªæ§åˆ¶å°å¤„ç†å™¨ï¼Œç”¨äºæ˜¾ç¤ºè­¦å‘Šå’Œé”™è¯¯
    console = logging.StreamHandler()
    console.setLevel(logging.WARNING)
    formatter = logging.Formatter('%(levelname)s - %(message)s')
    console.setFormatter(formatter)
    logging.getLogger('').addHandler(console)

def setup_parser():
    """è®¾ç½®å’Œé…ç½®å‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(
        description='æ ¹æ®JSONåˆ†æç»“æœç­›é€‰å›¾ç‰‡å¹¶å¤åˆ¶æ–‡ä»¶ã€‚',
        formatter_class=argparse.RawTextHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  - ç­›é€‰åˆ†æ•°å¤§äº8.5åˆ†çš„éAIã€æ— æ°´å°å›¾ç‰‡:
    python %(prog)s --source ./images --dest ./high_quality --score '>:8.5' --is-ai false --has-watermark false

  - ç­›é€‰åˆ†æ•°åœ¨7åˆ°8ä¹‹é—´ï¼Œæˆ–è€…æ˜¯AIç”Ÿæˆçš„å›¾ç‰‡:
    python %(prog)s --source ./images --dest ./filtered --score 'between:7:8' --is-ai true --logic OR
    
  - æ¨¡æ‹Ÿè¿è¡Œï¼ŒæŸ¥çœ‹å°†è¦å¤åˆ¶çš„æ–‡ä»¶:
    python %(prog)s --source ./images --dest ./filtered --score '<:5' --dry-run
"""
    )
    
    # å¿…éœ€å‚æ•°
    parser.add_argument('--source', type=str, required=True, help='åŒ…å«å›¾ç‰‡å’ŒJSONæ–‡ä»¶çš„æºç›®å½•è·¯å¾„ã€‚')
    parser.add_argument('--dest', type=str, required=True, help='ç”¨äºå­˜æ”¾ç­›é€‰åæ–‡ä»¶çš„ç›®æ ‡ç›®å½•è·¯å¾„ã€‚')
    
    # å¯é€‰ç­›é€‰å‚æ•°
    parser.add_argument('--score', type=str, help="åˆ†æ•°ç­›é€‰æ¡ä»¶ã€‚æ ¼å¼: 'OP:VALUE' æˆ– 'between:MIN:MAX'ã€‚\næœ‰æ•ˆOP: '>', '<', '==', '>=', '<='ã€‚ç¤ºä¾‹: --score '>:8.5'")
    parser.add_argument('--is-ai', type=str, choices=['true', 'false'], help="AIç”ŸæˆçŠ¶æ€ç­›é€‰ã€‚'true' æˆ– 'false'ã€‚")
    parser.add_argument('--has-watermark', type=str, choices=['true', 'false'], help="æ°´å°çŠ¶æ€ç­›é€‰ã€‚'true' æˆ– 'false'ã€‚")
    
    # æ§åˆ¶å‚æ•°
    parser.add_argument('--logic', type=str, choices=['AND', 'OR'], default='AND', help="å¤šä¸ªç­›é€‰æ¡ä»¶ä¹‹é—´çš„é€»è¾‘å…³ç³» (é»˜è®¤: AND)ã€‚")
    parser.add_argument('--workers', type=int, default=os.cpu_count(), help='å¹¶è¡Œå¤„ç†çš„åç¨‹æ•°é‡ (é»˜è®¤: ç³»ç»ŸCPUæ ¸å¿ƒæ•°)ã€‚')
    parser.add_argument('--dry-run', action='store_true', help='æ¨¡æ‹Ÿè¿è¡Œï¼Œåªæ‰“å°æ“ä½œä¿¡æ¯è€Œä¸å®é™…å¤åˆ¶æ–‡ä»¶ã€‚')
    parser.add_argument('--flat-output', action='store_true', help='å°†æ‰€æœ‰æ–‡ä»¶å¤åˆ¶åˆ°ç›®æ ‡ç›®å½•çš„æ ¹çº§åˆ«ï¼Œå¹¶ä»¥SHA256é‡å‘½åã€‚')
    parser.add_argument('--log-file', type=str, default='filter_log.txt', help='æŒ‡å®šæ—¥å¿—æ–‡ä»¶çš„è·¯å¾„ (é»˜è®¤: filter_log.txt)ã€‚')
    
    return parser

async def process_images_concurrent(
    image_pairs: List[Tuple[str, str]],
    args,
    max_concurrent: int = 8192
) -> Dict[str, int]:
    """
    å¹¶å‘å¤„ç†å›¾ç‰‡æ–‡ä»¶å¯¹

    Args:
        image_pairs: å›¾ç‰‡-JSONæ–‡ä»¶å¯¹åˆ—è¡¨
        args: å‘½ä»¤è¡Œå‚æ•°
        max_concurrent: æœ€å¤§å¹¶å‘æ•°

    Returns:
        Dict[str, int]: å¤„ç†ç»“æœç»Ÿè®¡
    """
    if not image_pairs:
        return {'copied': 0, 'skipped': 0, 'error': 0}

    # åˆå§‹åŒ–ä»»åŠ¡æ± 
    task_pool = BatchTaskPool(max_concurrent=max_concurrent)
    results = []
    pending_tasks = {}

    print(f"ASYNC: {max_concurrent} coroutines")

    # åˆå§‹åŒ–è®¡æ•°å™¨
    success_count = 0
    skip_count = 0
    error_count = 0

    # ä½¿ç”¨tqdmæ˜¾ç¤ºå¤„ç†è¿›åº¦
    with tqdm(total=len(image_pairs), desc="PROCESS", unit="pairs", ncols=80) as pbar:

        # æäº¤æ‰€æœ‰ä»»åŠ¡
        for img_path, json_path in image_pairs:
            coro = process_image_async(img_path, json_path, args)
            task_data = {"path": img_path}

            task_id, task = await task_pool.submit_task(coro, task_data)
            pending_tasks[task_id] = {"task": task, "data": task_data}

        # æ”¶é›†å®Œæˆçš„ä»»åŠ¡
        while pending_tasks:
            completed_task_ids = []

            for task_id, task_info in pending_tasks.items():
                if task_info["task"].done():
                    try:
                        result = await task_info["task"]
                        results.append(result)

                        # æ›´æ–°è¿›åº¦æ¡
                        pbar.update(1)

                        # æ›´æ–°è®¡æ•°å™¨å¹¶æ˜¾ç¤ºèšåˆç»Ÿè®¡
                        status = result.get("status", "unknown")
                        if status == "copied":
                            success_count += 1
                        elif status == "skipped":
                            skip_count += 1
                        else:
                            error_count += 1

                        # æ˜¾ç¤ºèšåˆç»Ÿè®¡
                        pbar.set_postfix_str(f"æˆåŠŸ={success_count}ï¼Œè·³è¿‡={skip_count}ï¼Œå¤±è´¥={error_count}")

                    except Exception as e:
                        # å¤„ç†ä»»åŠ¡å¼‚å¸¸
                        error_result = {
                            "status": "error",
                            "path": task_info["data"]["path"],
                            "details": f"ä»»åŠ¡æ‰§è¡Œé”™è¯¯: {str(e)}"
                        }
                        results.append(error_result)
                        pbar.update(1)

                        # æ›´æ–°é”™è¯¯è®¡æ•°å™¨å¹¶æ˜¾ç¤ºèšåˆç»Ÿè®¡
                        error_count += 1
                        pbar.set_postfix_str(f"æˆåŠŸ={success_count}ï¼Œè·³è¿‡={skip_count}ï¼Œå¤±è´¥={error_count}")

                    completed_task_ids.append(task_id)

            # ç§»é™¤å·²å®Œæˆçš„ä»»åŠ¡
            for task_id in completed_task_ids:
                pending_tasks.pop(task_id)

            # å¦‚æœè¿˜æœ‰æœªå®Œæˆçš„ä»»åŠ¡ï¼ŒçŸ­æš‚ç­‰å¾…
            if pending_tasks:
                await asyncio.sleep(0.1)

    # ç»Ÿè®¡ç»“æœ
    result_counts = {'copied': 0, 'skipped': 0, 'error': 0}
    for result in results:
        status = result.get("status", "error")
        if status in result_counts:
            result_counts[status] += 1
        else:
            result_counts['error'] += 1

    # æ˜¾ç¤ºä»»åŠ¡æ± ç»Ÿè®¡
    stats = task_pool.get_stats()
    print(f"STATS: {stats['success_rate']:.1f}% success ({stats['completed']}/{stats['total_submitted']})")

    return result_counts

async def main_async():
    """å¼‚æ­¥ä¸»å‡½æ•°ï¼Œå®ç°æµå¼è¿›åº¦æ¡å’Œå¹¶å‘å¤„ç†"""
    parser = setup_parser()
    args = parser.parse_args()
    
    # è·å–å®é™…çš„å¹¶å‘é…ç½®
    max_concurrent = int(os.getenv('IMAGE_FILTER_CONCURRENT_LIMIT', '8192'))

    # DOSé£æ ¼é…ç½®æ˜¾ç¤º
    print("=" * 60)
    print("IMAGE FILTER v2.0 - ASYNC EDITION")
    print("=" * 60)
    print(f"SOURCE: {args.source}")
    print(f"DEST  : {args.dest}")
    print(f"FILTER: {args.score or 'NONE'} | AI:{args.is_ai or 'ANY'} | WM:{args.has_watermark or 'ANY'}")
    print(f"ASYNC : {max_concurrent} COROUTINES")
    if args.dry_run:
        print("MODE  : DRY RUN (SIMULATION)")
    print("=" * 60)

    # é…ç½®æ—¥å¿—
    setup_logging(args.log_file)

    # æ£€æŸ¥ç­›é€‰æ¡ä»¶
    if not any([args.score, args.is_ai, args.has_watermark]):
        print("WARNING: No filter conditions specified!")
        return

    # æ–‡ä»¶å‘ç°é˜¶æ®µ
    print("\nSCANNING...")
    image_pairs = []
    discovered_count = 0

    # ä½¿ç”¨æµå¼å‘ç°å¹¶æ˜¾ç¤ºå®æ—¶è¿›åº¦
    with tqdm(desc="DISCOVER", unit="pairs", ncols=80) as discovery_pbar:
        async for img_path, json_path in discover_image_json_pairs_streaming(args.source):
            image_pairs.append((img_path, json_path))
            discovered_count += 1
            discovery_pbar.update(1)

            # æ¯å‘ç°1000ä¸ªæ–‡ä»¶å°±è®©å‡ºæ§åˆ¶æƒ
            if discovered_count % 1000 == 0:
                await asyncio.sleep(0)

    if not image_pairs:
        print("ERROR: No image-JSON pairs found!")
        return

    print(f"FOUND: {len(image_pairs)} pairs")

    # å¤„ç†é˜¶æ®µ
    print("PROCESSING...")
    results = await process_images_concurrent(
        image_pairs,
        args,
        max_concurrent=max_concurrent
    )

    # ç»“æœç»Ÿè®¡
    print("\n" + "=" * 60)
    print("RESULTS:")
    print(f"COPIED : {results['copied']}")
    print(f"SKIPPED: {results['skipped']}")
    print(f"ERRORS : {results['error']}")
    print(f"LOG    : {args.log_file}")
    if args.dry_run:
        print("STATUS : SIMULATION COMPLETE")
    else:
        print("STATUS : OPERATION COMPLETE")
    print("=" * 60)

def main():
    """åŒæ­¥å…¥å£ç‚¹ï¼Œè°ƒç”¨å¼‚æ­¥ä¸»å‡½æ•°"""
    return asyncio.run(main_async())

if __name__ == "__main__":
    main()